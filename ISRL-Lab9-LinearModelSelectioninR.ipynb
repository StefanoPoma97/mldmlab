{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Model Validation using Validation and Cross-Validation in R comes from p. 248-251 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was re-implemented in Fall 2016 in `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "library(leaps)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using the Validation Set Approach\n",
    "\n",
    "In Lab 8, we saw that it is possible to choose among a set of models of different\n",
    "sizes using $C_p$, BIC, and adjusted $R^2$. We will now consider how to do this\n",
    "using the validation set and cross-validation approaches.\n",
    "\n",
    "As in Lab 8, we'll be working with the `Hitters` dataset from `ISLR`. Since we're trying to predict `Salary` and we know from last time that some are missing, let's first drop all the rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hitters = na.omit(Hitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for these approaches to yield accurate estimates of the test\n",
    "error, we must use *only the training observations* to perform all aspects of\n",
    "model-fitting â€” including variable selection. Therefore, the determination of\n",
    "which model of a given size is best must be made using *only the training\n",
    "observations*. This point is subtle but important. If the full data set is used\n",
    "to perform the best subset selection step, the validation set errors and\n",
    "cross-validation errors that we obtain will not be accurate estimates of the\n",
    "test error.\n",
    "\n",
    "In order to use the validation set approach, we begin by splitting the\n",
    "observations into a training set and a test set as before. Here, we've decided to split the data in half using the `sample_frac()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "\n",
    "train = Hitters %>%\n",
    "  sample_frac(0.5)\n",
    "\n",
    "test = Hitters %>%\n",
    "  setdiff(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply `regsubsets()` to the training set in order to perform best\n",
    "subset selection\\*.\n",
    "\n",
    "( \\*Note: If you're trying to complete this lab on a machine that can't handle calculating the **best subset**, or if you just want it to run a little faster, try forward or backward selection instead by adding the `method = \"forward\"` or `method = \"backward\"` parameter to your call to `regsubsets()`. You'll get slightly different values, but the concepts are the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regfit_best_train = regsubsets(Salary~., data = train, nvmax = 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we subset the `Hitters` data frame directly in the call in order\n",
    "to access only the training subset of the data, using the expression\n",
    "`Hitters[train,]`. We now compute the validation set error for the best\n",
    "model of each model size. We first make a model matrix from the test\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_mat = model.matrix (Salary~., data = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.matrix()` function is used in many regression packages for building an $X$ matrix from data. Now we run a loop, and for each size $i$, we\n",
    "extract the coefficients from `regfit.best` for the best model of that size,\n",
    "multiply them into the appropriate columns of the test model matrix to\n",
    "form the predictions, and compute the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_errors = rep(NA,19)\n",
    "\n",
    "# Iterates over each size i\n",
    "for(i in 1:19){\n",
    "    \n",
    "    # Extract the vector of predictors in the best fit model on i predictors\n",
    "    coefi = coef(regfit_best_train, id = i)\n",
    "    \n",
    "    # Make predictions using matrix multiplication of the test matirx and the coefficients vector\n",
    "    pred = test_mat[,names(coefi)]%*%coefi\n",
    "    \n",
    "    # Calculate the MSE\n",
    "    val_errors[i] = mean((test$Salary-pred)^2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the errors, and find the model that minimizes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the model with the smallest error\n",
    "min = which.min(val_errors)\n",
    "\n",
    "# Plot the errors for each model size\n",
    "plot(val_errors, type = 'b')\n",
    "points(min, val_errors[min][1], col = \"red\", cex = 2, pch = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! We find that the best model (according to the validation set approach) is the one that contains 10 predictors.\n",
    "\n",
    "This was a little tedious, partly because there is no `predict()` method\n",
    "for `regsubsets()`. Since we will be using this function again, we can capture\n",
    "our steps above and write our own `predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict.regsubsets = function(object,newdata,id,...){\n",
    "      form = as.formula(object$call[[2]]) # Extract the formula used when we called regsubsets()\n",
    "      mat = model.matrix(form,newdata)    # Build the model matrix\n",
    "      coefi = coef(object,id=id)          # Extract the coefficiants of the ith model\n",
    "      xvars = names(coefi)                # Pull out the names of the predictors used in the ith model\n",
    "      mat[,xvars]%*%coefi               # Make predictions using matrix multiplication\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pretty much mimics what we did above. The one tricky\n",
    "part is how we extracted the formula used in the call to `regsubsets()`, but you don't need to worry too much about the mechanics of this right now. We'll use this function to make our lives a little easier when we do cross-validation.\n",
    "\n",
    "Now that we know what we're looking for, let's perform best subset selection on the full dataset (up to 10 predictors) and select the best 10-predictor model. It is important that we make use of the *full\n",
    "data set* in order to obtain more accurate coefficient estimates. Note that\n",
    "we perform best subset selection on the full data set and select the best 10-predictor\n",
    "model, rather than simply using the predictors that we obtained\n",
    "from the training set, because the best 10-predictor model on the **full data\n",
    "set** may differ from the corresponding model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regfit_best = regsubsets(Salary~., data = Hitters, nvmax = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we see that the best ten-variable model on the full data set has a\n",
    "**different set of predictors** than the best ten-variable model on the training\n",
    "set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef(regfit_best, 10)\n",
    "coef(regfit_best_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using Cross-Validation\n",
    "\n",
    "Now let's try to choose among the models of different sizes using cross-validation.\n",
    "This approach is somewhat involved, as we must perform best\n",
    "subset selection\\* within each of the $k$ training sets. Despite this, we see that\n",
    "with its clever subsetting syntax, `R` makes this job quite easy. First, we\n",
    "create a vector that assigns each observation to one of $k = 10$ folds, and\n",
    "we create a matrix in which we will store the results:\n",
    "\n",
    "\\* or forward selection / backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 10        # number of folds\n",
    "set.seed(1)   # set the random seed so we all get the same results\n",
    "\n",
    "# Assign each observation to a single fold\n",
    "folds = sample(1:k, nrow(Hitters), replace = TRUE)\n",
    "\n",
    "# Create a matrix to store the results of our upcoming calculations\n",
    "cv_errors = matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a for loop that performs cross-validation. In the $j^{th}$ fold, the\n",
    "elements of folds that equal $j$ are in the test set, and the remainder are in\n",
    "the training set. We make our predictions for each model size (using our\n",
    "new $predict()$ method), compute the test errors on the appropriate subset,\n",
    "and store them in the appropriate slot in the matrix `cv.errors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outer loop iterates over all folds\n",
    "for(j in 1:k){\n",
    "    \n",
    "    # The perform best subset selection on the full dataset, minus the jth fold\n",
    "    best_fit = regsubsets(Salary~., data = Hitters[folds!=j,], nvmax=19)\n",
    "    \n",
    "    # Inner loop iterates over each size i\n",
    "    for(i in 1:19){\n",
    "        \n",
    "        # Predict the values of the current fold from the \"best subset\" model on i predictors\n",
    "        pred = predict(best_fit, Hitters[folds==j,], id=i)\n",
    "        \n",
    "        # Calculate the MSE, store it in the matrix we created above\n",
    "        cv_errors[j,i] = mean((Hitters$Salary[folds==j]-pred)^2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has filled up the `cv_._errors` matrix such that the $(i,j)^{th}$ element corresponds\n",
    "to the test MSE for the $i^{th}$ cross-validation fold for the best $j$-variable\n",
    "model.  We can then use the `apply()` function to take the `mean` over the columns of this\n",
    "matrix. This will give us a vector for which the $j^{th}$ element is the cross-validation\n",
    "error for the $j$-variable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the mean of over all folds for each model size\n",
    "mean_cv_errors = apply(cv_errors, 2, mean)\n",
    "\n",
    "# Find the model size with the smallest cross-validation error\n",
    "min = which.min(mean_cv_errors)\n",
    "\n",
    "# Plot the cross-validation error for each model size, highlight the min\n",
    "plot(mean_cv_errors, type='b')\n",
    "points(min, mean_cv_errors[min][1], col = \"red\", cex = 2, pch = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that cross-validation selects an 11-predictor model. Now let's use\n",
    "best subset selection on the full data set in order to obtain the 11-predictor\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_best = regsubsets(Salary~., data = Hitters, nvmax = 19)\n",
    "coef(reg_best, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's also take a look at the statistics from last lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "\n",
    "reg_summary = summary(reg_best)\n",
    "\n",
    "# Plot RSS\n",
    "plot(reg_summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\n",
    "\n",
    "# Plot Adjusted R^2, highlight max value\n",
    "plot(reg_summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\n",
    "max = which.max(reg_summary$adjr2)\n",
    "points(max, reg_summary$adjr2[max], col = \"red\", cex = 2, pch = 20)\n",
    "\n",
    "# Plot Cp, highlight min value\n",
    "plot(reg_summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\n",
    "min = which.min(reg_summary$cp)\n",
    "points(min,reg_summary$cp[min], col = \"red\", cex = 2, pch = 20)\n",
    "\n",
    "# Plot BIC, highlight min value\n",
    "plot(reg_summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\n",
    "min = which.min(reg_summary$bic)\n",
    "points(min, reg_summary$bic[min], col = \"red\", cex = 2, pch = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how some of the indicators agree with the cross-validated model, and others are very different?\n",
    "\n",
    "# Your turn!\n",
    "\n",
    "Now it's time to test out these approaches (best / forward / backward selection) and evaluation methods (adjusted training error, validation set, cross validation) on other datasets. You may want to work with a team on this portion of the lab.\n",
    "\n",
    "You may use any of the datasets included in the `ISLR` package, or choose one from the UCI machine learning repository (http://archive.ics.uci.edu/ml/datasets.html). Download a dataset, and try to determine the optimal set of parameters to use to model it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get credit for this lab, please post your answers to the following questions:\n",
    "\n",
    "* What dataset did you choose?\n",
    "* Which selection techniques did you try?\n",
    "* Which evaluation techniques did you try?\n",
    "* What did you determine was the best set of parameters to model this data?\n",
    "* How well did this model perform?\n",
    "\n",
    "to Moodle: https://moodle.smith.edu/mod/quiz/view.php?id=258534"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
