{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Decision Trees in R is an abbreviated version of p. 324-331 of \"Introduction to Statistical Learning with\n",
    "Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was re-implemented in Fall 2016 in `tidyverse` format by Amelia McNamara and R. Jordan Crouser at Smith College.\n",
    "\n",
    "# 8.3.1 Fitting Classification Trees\n",
    "\n",
    "The `tree` library is useful for constructing classification and regression trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(tree)\n",
    "library(ISLR)\n",
    "library(dplyr)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using **classification trees** to analyze the `Carseats` data set. In these\n",
    "data, `Sales` is a continuous variable, and so we begin by converting it to a\n",
    "binary variable. We use the `ifelse()` function to create a variable, called\n",
    "`High`, which takes on a value of `Yes` if the `Sales` variable exceeds 8, and\n",
    "takes on a value of `No` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Carseats = Carseats %>%\n",
    "  mutate(High = as.factor(ifelse(Sales <= 8, \"No\", \"Yes\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly evaluate the performance of a classification tree on\n",
    "the data, we must estimate the test error rather than simply computing\n",
    "the training error. We first split the observations into a training set and a test\n",
    "set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "\n",
    "train = Carseats %>%\n",
    "  sample_n(200)\n",
    "\n",
    "test = Carseats %>%\n",
    "  setdiff(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the `tree()` function to fit a classification tree in order to predict\n",
    "`High` using all variables but `Sales` (that would be a little silly...). The syntax of the `tree()` function is quite\n",
    "similar to that of the `lm()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_carseats = tree(High ~ . -Sales, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `summary()` function lists the variables that are used as internal nodes (forming decision points)\n",
    "in the tree, the number of terminal nodes, and the (training) error rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(tree_carseats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training error rate 9%. For classification trees, the `deviance`\n",
    "reported in the output of `summary()` is given by:\n",
    "\n",
    "$$-2\\sum_m\\sum_k n_{mk}log\\hat{p}_{mk}$$\n",
    "\n",
    "where $n_{mk}$ is the number of observations in the $m^{th}$ terminal node that\n",
    "belong to the $k^{th}$ class. A small `deviance` indicates a tree that provides\n",
    "a good fit to the (training) data. The `residual \\ mean \\ deviance` reported is\n",
    "simply the `deviance` divided by $nâˆ’|T_0|$.\n",
    "\n",
    "One of the most attractive properties of trees is that they can be\n",
    "graphically displayed. We use the `plot()` function to display the tree structure,\n",
    "and the `text()` function to display the node labels. The argument\n",
    "`pretty = 0` instructs `R` to include the category names for any qualitative predictors,\n",
    "rather than simply displaying a letter for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(tree_carseats)\n",
    "text(tree_carseats, pretty = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important indicator of `High` sales appears to be shelving location,\n",
    "since the first branch differentiates `Good` locations from `Bad` and `Medium`\n",
    "locations.\n",
    "\n",
    "If we just type the name of the tree object, `R` prints output corresponding\n",
    "to each branch of the tree. `R` displays the split criterion (e.g. $Price<142$), the\n",
    "number of observations in that branch, the deviance, the overall prediction\n",
    "for the branch (`Yes` or `No`), and the fraction of observations in that branch\n",
    "that take on values of `Yes` and `No`. Branches that lead to terminal nodes are\n",
    "indicated using asterisks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_carseats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the tree's performance on\n",
    "the test data. The `predict()` function can be used for this purpose. In the\n",
    "case of a classification tree, the argument `type=\"class\"` instructs `R` to return\n",
    "the actual class prediction. This approach leads to correct predictions for\n",
    "around 77% of the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_pred = predict(tree_carseats, test, type = \"class\")\n",
    "table(tree_pred, test$High)\n",
    "# (98+56)/200 = 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "\n",
    "Next, we consider whether **pruning** the tree might lead to improved\n",
    "results. The function `cv.tree()` performs cross-validation in order to\n",
    "determine the optimal level of tree complexity; cost complexity pruning\n",
    "is used in order to select a sequence of trees for consideration. We use\n",
    "the argument `FUN = prune.misclass` in order to indicate that we want the\n",
    "**classification error rate** as our cost function to guide the cross-validation and pruning process,\n",
    "rather than the default for the `cv.tree()` function, which is `deviance`. The\n",
    "`cv.tree()` function reports the number of terminal nodes of each tree considered\n",
    "(size) as well as the corresponding error rate and the value of the\n",
    "cost-complexity parameter used ($k$, which corresponds to $\\alpha$ in the equation we saw in lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(3)\n",
    "cv_carseats = cv.tree(tree_carseats, FUN = prune.misclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, despite the name, the `dev` field corresponds to the **cross-validation error\n",
    "rate** in this instance. Let's plot the error\n",
    "rate as a function of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(cv_carseats$size, cv_carseats$dev, type = \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this plot that the tree with 7 terminal nodes results in the lowest\n",
    "cross-validation error rate, with 59 cross-validation errors. \n",
    "\n",
    "We now apply the `prune.misclass()` function in order to prune the tree to\n",
    "obtain the nine-node tree by setting the parameter `best = 7`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prune_carseats = prune.misclass(tree_carseats, best = 7)\n",
    "plot(prune_carseats)\n",
    "text(prune_carseats, pretty = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this pruned tree perform on the test data set? Once again,\n",
    "we can apply the `predict()` function top find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_pred = predict(prune_carseats, test, type = \"class\")\n",
    "table(tree_pred, test$High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $\\frac{(96+54)}{200} =$ 75% of the test observations are correctly classified, so \n",
    "the pruning process produced a more interpretable tree, but at a slight cost in classification accuracy.\n",
    "\n",
    "# 8.3.2 Fitting Regression Trees\n",
    "\n",
    "Now let's try fitting a **regression tree** to the `Boston` data set from the `MASS` library. First, we create a\n",
    "training set, and fit the tree to the training data using `medv` (median home value) as our response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "set.seed(1)\n",
    "boston_train = Boston %>%\n",
    "  sample_frac(.5)\n",
    "\n",
    "boston_test = Boston %>%\n",
    "  setdiff(boston_train)\n",
    "\n",
    "tree_boston=tree(medv~., boston_train)\n",
    "\n",
    "summary(tree_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of `summary()` indicates that only three of the variables\n",
    "have been used in constructing the tree_ In the context of a regression\n",
    "tree, the `deviance` is simply the sum of squared errors for the tree_ Let's\n",
    "plot the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(tree_boston)\n",
    "text(tree_boston, pretty = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `lstat` measures the percentage of individuals with lower\n",
    "socioeconomic status. The tree indicates that lower values of `lstat` correspond\n",
    "to more expensive houses. The tree predicts a median house price\n",
    "of \\$46,380 for larger homes ($rm>=7.437$) in suburbs in which residents have high socioeconomic\n",
    "status ($lstat<9.715$).\n",
    "\n",
    "Now we use the `cv.tree()` function to see whether pruning the tree will\n",
    "improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_boston = cv.tree(tree_boston)\n",
    "plot(cv_boston$size, cv_boston$dev, type = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7-node tree is selected by cross-validation. We can prune the tree using the\n",
    "`prune.tree()` function as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prune_boston = prune.tree(tree_boston, \n",
    "                          best = 7)\n",
    "plot(prune_boston)\n",
    "text(prune_boston, pretty = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the pruned tree to make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_tree_estimate = predict(prune_boston, \n",
    "                               newdata = boston_train)\n",
    "\n",
    "ggplot() + \n",
    "    geom_point(aes(x = boston_test$medv, y = single_tree_estimate)) +\n",
    "    geom_abline()\n",
    "\n",
    "mean((single_tree_estimate - boston_test$medv)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the test set MSE associated with the regression tree is\n",
    "154.4729. The square root of the MSE is therefore around 12.428, indicating\n",
    "that this model leads to test predictions that are within around \\$12,428 of\n",
    "the true median home value for the suburb.\n",
    "    \n",
    "# 8.3.3 Bagging and Random Forests\n",
    "\n",
    "Let's see if we can improve on this result using **bagging** and **random forests**. The exact results obtained in this section may\n",
    "depend on the version of `R` and the version of the `randomForest` package\n",
    "installed on your computer, so don't stress out if you don't match up exactly with the book. Recall that **bagging** is simply a special case of\n",
    "a **random forest** with $m = p$. Therefore, the `randomForest()` function can\n",
    "be used to perform both random forests and bagging. Let's start with bagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "set.seed(1)\n",
    "bag_boston = randomForest(medv~., \n",
    "                          data = boston_train, \n",
    "                          mtry = 13, \n",
    "                          importance = TRUE)\n",
    "bag_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `mtry = 13` indicates that all 13 predictors should be considered\n",
    "for each split of the tree -- in other words, that bagging should be done. How\n",
    "well does this bagged model perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bagged_estimate = predict(bag_boston, \n",
    "                          newdata = boston_test)\n",
    "\n",
    "ggplot() + \n",
    "    geom_point(aes(x = boston_test$medv, y = bagged_estimate)) +\n",
    "    geom_abline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the bagged regression tree is dramatically smaller than that obtained using an optimally-pruned single tree! We can change\n",
    "the number of trees grown by `randomForest()` using the `ntree` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_boston_25_trees = randomForest(medv~., data =  boston_train, mtry = 13, ntree = 25)\n",
    "bagged_estimate_25_trees = predict(bag_boston_25_trees, newdata = boston_test)\n",
    "mean((bagged_estimate_25_trees - boston_test$medv)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grow a random forest in exactly the same way, except that\n",
    "we'll use a smaller value of the `mtry` argument. By default, `randomForest()`\n",
    "uses $p/3$ variables when building a random forest of regression trees, and\n",
    "$\\sqrt{p` variables when building a random forest of classification trees. Here we'll\n",
    "use `mtry = 6`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "rf_boston = randomForest(medv~., \n",
    "                         data = boston_train, \n",
    "                         mtry = 6, \n",
    "                         importance = TRUE)\n",
    "\n",
    "random_forest_estimate = predict(rf_boston, \n",
    "                                 newdata = boston_test)\n",
    "\n",
    "mean((random_forest_estimate - boston_test$medv)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE is even lower; this indicates that random forests yielded an\n",
    "improvement over bagging in this case.\n",
    "\n",
    "Using the `importance()` function, we can view the importance of each\n",
    "variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance(rf_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two measures of variable importance are reported. The former is based\n",
    "upon the **mean decrease of accuracy in predictions** on the out-of-bag samples\n",
    "when a given variable is excluded from the model. The latter is a measure\n",
    "of the **total decrease in node impurity** that results from splits over that\n",
    "variable, averaged over all tree_ In the\n",
    "case of regression trees, the node impurity is measured by the training\n",
    "RSS, and for classification trees by the deviance. Plots of these importance\n",
    "measures can be produced using the `varImpPlot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "varImpPlot(rf_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that across all of the trees considered in the random\n",
    "forest, the wealth level of the community (`lstat`) and the house size (`rm`)\n",
    "are by far the two most important variables.\n",
    "\n",
    "# 8.3.4 Boosting\n",
    "\n",
    "Now we'll use the `gbm` package, and within it the `gbm()` function, to fit **boosted\n",
    "regression trees** to the `Boston` data set. We run `gbm()` with the option\n",
    "`distribution=\"gaussian\"` since this is a regression problem; if it were a binary\n",
    "classification problem, we would use `distribution=\"bernoulli\"`. The\n",
    "argument `n.trees=5000` indicates that we want 5000 trees, and the option\n",
    "`interaction.depth=4` limits the depth of each tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(gbm)\n",
    "set.seed(1)\n",
    "boost_boston = gbm(medv~., \n",
    "                   data = boston_train, \n",
    "                   distribution = \"gaussian\", \n",
    "                   n.trees = 5000, \n",
    "                   interaction.depth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `summary()` function produces a relative influence plot and also outputs\n",
    "the relative influence statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(boost_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `lstat` and `rm` are again the most important variables by far. We can\n",
    "also produce partial dependence plots for these two variables. These plots\n",
    "illustrate the marginal effect of the selected variables on the response after\n",
    "integrating out the other variables. In this case, as we might expect, median\n",
    "house prices are increasing with `rm` and decreasing with `lstat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par(mfrow = c(1,2))\n",
    "plot(boost_boston, i = \"rm\")\n",
    "plot(boost_boston, i = \"lstat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the boosted model to predict `medv` on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boost_estimate = predict(boost_boston, \n",
    "                         newdata = boston_test, \n",
    "                         n.trees = 5000)\n",
    "\n",
    "mean((boost_estimate - boston_test$medv)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE obtained is similar to the test MSE for random forests\n",
    "and bagging. If we want to, we can perform boosting\n",
    "with a different value of the shrinkage parameter $\\lambda$. The default\n",
    "value is 0.001, but this is easily modified. Here we take $\\lambda = 0.1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boost_boston2 = gbm(medv~., data = boston_train, \n",
    "                    distribution = \"gaussian\", \n",
    "                    n.trees = 5000, \n",
    "                    interaction.depth = 4, \n",
    "                    shrinkage = 0.01, \n",
    "                    verbose = F)\n",
    "\n",
    "boost_estimate2 = predict(boost_boston2, newdata = boston_test, n.trees = 5000)\n",
    "mean((boost_estimate2-boston_test$medv)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using $\\lambda = 0.1$ leads to a slightly lower test MSE than $\\lambda = 0.001$.\n",
    "\n",
    "To get credit for this lab, post your responses to the following questions:\n",
    " - What's one real-world scenario where you might try using Bagging?\n",
    " - What's one real-world scenario where you might try using Random Forests?\n",
    " - What's one real-world scenario where you might try using Boosting?\n",
    " \n",
    "to Moodle: https://moodle.smith.edu/mod/quiz/view.php?id=264671"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
