{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cluster validation statistics in R\n",
    "Required R packages\n",
    "\n",
    "The following R packages are required in this chapter:\n",
    "\n",
    "*    factoextra for data visualization\n",
    "*    fpc for computing clustering validation statistics\n",
    "*    NbClust for determining the optimal number of clusters in the data set.\n",
    "\n",
    "Install and load the packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list.of.packages <- c(\"factoextra\", \"fpc\", \"NbClust\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(factoextra)\n",
    "library(fpc)\n",
    "library(NbClust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation\n",
    "\n",
    "We’ll use the built-in R data set iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the column \"Species\" at position 5\n",
    "df <- iris[, -5]\n",
    "# Standardize\n",
    "df <- scale(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering analysis\n",
    "\n",
    "We’ll use the function eclust() [enhanced clustering, in factoextra] which provides several advantages:\n",
    "\n",
    "*    It simplifies the workflow of clustering analysis\n",
    "*    It can be used to compute hierarchical clustering and partitioning clustering in a single line function call\n",
    "*    Compared to the standard partitioning functions (kmeans, pam, clara and fanny) which requires the user to specify the optimal number of clusters, the function eclust() computes automatically the gap statistic for estimating the right number of clusters.\n",
    "*    It provides silhouette information for all partitioning methods and hierarchical clustering\n",
    "*    It draws beautiful graphs using ggplot2\n",
    "\n",
    "The simplified format the eclust() function is as follow:\n",
    "\n",
    "eclust(x, FUNcluster = \"kmeans\", hc_metric = \"euclidean\", ...)\n",
    "\n",
    "\n",
    "\n",
    "*    x: numeric vector, data matrix or data frame\n",
    "*    FUNcluster: a clustering function including “kmeans”, “pam”, “clara”, “fanny”, “hclust”, “agnes” and “diana”. Abbreviation is allowed.\n",
    "*    hc_metric: character string specifying the metric to be used for calculating dissimilarities between observations. Allowed values are those accepted by the function dist() [including “euclidean”, “manhattan”, “maximum”, “canberra”, “binary”, “minkowski”] and correlation based distance measures [“pearson”, “spearman” or “kendall”]. Used only when FUNcluster is a hierarchical clustering function such as one of “hclust”, “agnes” or “diana”.\n",
    "*    …: other arguments to be passed to FUNcluster.\n",
    "\n",
    "The function eclust() returns an object of class eclust containing the result of the standard function used (e.g., kmeans, pam, hclust, agnes, diana, etc.).\n",
    "\n",
    "It includes also:\n",
    "\n",
    "*    cluster: the cluster assignment of observations after cutting the tree\n",
    "*    nbclust: the number of clusters\n",
    "*    silinfo: the silhouette information of observations\n",
    "*    size: the size of clusters\n",
    "*    data: a matrix containing the original or the standardized data (if stand = TRUE)\n",
    "*    gap_stat: containing gap statistics\n",
    "\n",
    "To compute a partitioning clustering, such as k-means clustering with k = 3, type this:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "km.res <- eclust(df, \"kmeans\", k = 3, nstart = 25, graph = FALSE)\n",
    "# Visualize k-means clusters\n",
    "fviz_cluster(km.res, geom = \"point\", ellipse.type = \"norm\",\n",
    "             palette = \"jco\", ggtheme = theme_minimal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute a hierarchical clustering, use this:\n",
    "\n",
    "# Hierarchical clustering\n",
    "hc.res <- eclust(df, \"hclust\", k = 3, hc_metric = \"euclidean\", \n",
    "                 hc_method = \"ward.D2\", graph = FALSE)\n",
    "\n",
    "# Visualize dendrograms\n",
    "fviz_dend(hc.res, show_labels = FALSE,\n",
    "         palette = \"jco\", as.ggplot = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering validation\n",
    "\n",
    "# Silhouette plot\n",
    "\n",
    "Recall that the silhouette coefficient (Si) measures how similar an object i is to the the other objects in its own cluster versus those in the neighbor cluster. Si\n",
    "\n",
    "values range from 1 to - 1:\n",
    "\n",
    "*    A value of Si close to 1 indicates that the object is well clustered. In the other words, the object i is similar to the other objects in its group.\n",
    "* A value of Si close to -1 indicates that the object is poorly clustered, and that assignment to some other cluster would probably improve the overall results.\n",
    "\n",
    "It’s possible to draw silhouette coefficients of observations using the function fviz_silhouette() [factoextra package], which will also print a summary of the silhouette analysis output. To avoid this, you can use the option print.summary = FALSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_silhouette(km.res, palette=\"jco\",ggtheme=theme_classic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette information can be extracted as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette information\n",
    "silinfo <- km.res$silinfo\n",
    "names(silinfo)\n",
    "# Silhouette widths of each observation\n",
    "head(silinfo$widths[, 1:3], 10)\n",
    "# Average silhouette width of each cluster\n",
    "silinfo$clus.avg.widths\n",
    "# The total average (mean of all individual silhouette widths)\n",
    "silinfo$avg.width\n",
    "# The size of each clusters\n",
    "km.res$size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that several samples, in cluster 2, have a negative silhouette coefficient. This means that they are not in the right cluster. We can find the name of these samples and determine the clusters they are closer (neighbor cluster), as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette width of observation\n",
    "sil <- km.res$silinfo$widths[, 1:3]\n",
    "# Objects with negative silhouette\n",
    "neg_sil_index <- which(sil[, 'sil_width'] < 0)\n",
    "sil[neg_sil_index, , drop = FALSE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validation statistics\n",
    "\n",
    "The function cluster.stats() [fpc package] and the function NbClust() [in NbClust package] can be used to compute Dunn index and many other cluster validation statistics or indices.\n",
    "\n",
    "The simplified format is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.stats(d = NULL, clustering, al.clustering = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*    d: a distance object between cases as generated by the dist() function\n",
    "*    clustering: vector containing the cluster number of each observation\n",
    "*    alt.clustering: vector such as for clustering, indicating an alternative clustering\n",
    "\n",
    "The function cluster.stats() returns a list containing many components useful for analyzing the intrinsic characteristics of a clustering:\n",
    "\n",
    "*    cluster.number: number of clusters\n",
    "*    cluster.size: vector containing the number of points in each cluster\n",
    "*    average.distance, median.distance: vector containing the cluster-wise within average/median distances\n",
    "*    average.between: average distance between clusters. We want it to be as large as possible\n",
    "*    average.within: average distance within clusters. We want it to be as small as possible\n",
    "*    clus.avg.silwidths: vector of cluster average silhouette widths. Recall that, the silhouette width is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.\n",
    "*    within.cluster.ss: a generalization of the within clusters sum of squares (k-means objective function), which is obtained if d is a Euclidean distance matrix.\n",
    "*    dunn, dunn2: Dunn index\n",
    "*    corrected.rand, vi: Two indexes to assess the similarity of two clustering: the corrected Rand index and Meila’s VI\n",
    "\n",
    "All the above elements can be used to evaluate the internal quality of clustering.\n",
    "\n",
    "In the following sections, we’ll compute the clustering quality statistics for k-means. Look at the within.cluster.ss (within clusters sum of squares), the average.within (average distance within clusters) and clus.avg.silwidths (vector of cluster average silhouette widths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(fpc)\n",
    "# Statistics for k-means clustering\n",
    "km_stats <- cluster.stats(dist(df),  km.res$cluster)\n",
    "# Dun index\n",
    "km_stats$dunn\n",
    "\n",
    "# To display all statistics, type this:\n",
    "\n",
    "km_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External clustering validation\n",
    "\n",
    "Among the values returned by the function cluster.stats(), there are two indexes to assess the similarity of two clustering, namely the corrected Rand index and Meila’s VI.\n",
    "\n",
    "We know that the iris data contains exactly 3 groups of species.\n",
    "\n",
    "Does the K-means clustering matches with the true structure of the data?\n",
    "\n",
    "We can use the function cluster.stats() to answer to this question.\n",
    "\n",
    "Let start by computing a cross-tabulation between k-means clusters and the reference Species labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(iris$Species, km.res$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that:\n",
    "\n",
    "*    All setosa species (n = 50) has been classified in cluster 1\n",
    "*    A large number of versicor species (n = 39 ) has been classified in cluster 3. Some of them ( n = 11) have been classified in cluster 2.\n",
    "*    A large number of virginica species (n = 36 ) has been classified in cluster 2. Some of them (n = 14) have been classified in cluster 3.\n",
    "\n",
    "It’s possible to quantify the agreement between Species and k-means clusters using either the corrected Rand index and Meila’s VI provided as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"fpc\")\n",
    "# Compute cluster stats\n",
    "species <- as.numeric(iris$Species)\n",
    "clust_stats <- cluster.stats(d = dist(df), \n",
    "                             species, km.res$cluster)\n",
    "# Corrected Rand index\n",
    "clust_stats$corrected.rand\n",
    "\n",
    "# VI\n",
    "clust_stats$vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The corrected Rand index provides a measure for assessing the similarity between two partitions, adjusted for chance. Its range is -1 (no agreement) to 1 (perfect agreement). Agreement between the specie types and the cluster solution is 0.62 using Rand index and 0.748 using Meila’s VI.\n",
    "\n",
    "The same analysis can be computed for both PAM and hierarchical clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement between species and pam clusters\n",
    "pam.res <- eclust(df, \"pam\", k = 3, graph = FALSE)\n",
    "table(iris$Species, pam.res$cluster)\n",
    "cluster.stats(d = dist(iris.scaled), \n",
    "              species, pam.res$cluster)$vi\n",
    "# Agreement between species and HC clusters\n",
    "res.hc <- eclust(df, \"hclust\", k = 3, graph = FALSE)\n",
    "table(iris$Species, res.hc$cluster)\n",
    "cluster.stats(d = dist(iris.scaled), \n",
    "              species, res.hc$cluster)$vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External clustering validation, can be used to select suitable clustering algorithm for a given data set.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We described how to validate clustering results using the silhouette method and the Dunn index. This task is facilitated using the combination of two R functions: eclust() and fviz_silhouette in the factoextra package We also demonstrated how to assess the agreement between a clustering result and an external reference.\n",
    "In the next chapters, we’ll show how to i) choose the appropriate clustering algorithm for your data; and ii) computing p-values for hierarchical clustering.\n",
    "\n",
    "\n",
    "## References\n",
    "*  Alboukadel Kassambara, 2017 \"Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning\"\n",
    "\n",
    "* Brock, Guy, Vasyl Pihur, Susmita Datta, and Somnath Datta. 2008. “ClValid: An R Package for Cluster Validation.” Journal of Statistical Software 25 (4): 1–22. https://www.jstatsoft.org/v025/i04.\n",
    "\n",
    "* Charrad, Malika, Nadia Ghazzali, Véronique Boiteau, and Azam Niknafs. 2014. “NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set.” Journal of Statistical Software 61: 1–36. http://www.jstatsoft.org/v61/i06/paper.\n",
    "\n",
    "* Theodoridis, Sergios, and Konstantinos Koutroumbas. 2008. Pattern Recognition. 2nd ed. Academic Press.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
