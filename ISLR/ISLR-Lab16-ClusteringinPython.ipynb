{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on K-Means and Hierarchical Clustering in R is an adaptation of p. 404-407, 410-413 of \"Introduction to\n",
    "Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert\n",
    "Tibshirani. Initial python implementation by [Hyun Bong Lee](https://github.com/hyunblee), adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Fall 2017).\n",
    "\n",
    "\n",
    "# 10.5.1 K-Means Clustering\n",
    "The `sklearn` function `Kmeans()` performs K-means clustering in R. We begin with\n",
    "a simple simulated example in which there truly are two clusters in the\n",
    "data: the first 25 observations have a mean shift relative to the next 25\n",
    "observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(123)\n",
    "X = np.random.randn(50,2)\n",
    "X[0:25, 0] = X[0:25, 0] + 3\n",
    "X[0:25, 1] = X[0:25, 1] - 4\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(X[:,0], X[:,1], s=50) \n",
    "ax.set_xlabel('X0')\n",
    "ax.set_ylabel('X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform K-means clustering with `K = 2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 123).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignments of the 50 observations are contained in\n",
    "`kmeans.labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means clustering perfectly separated the observations into two clusters\n",
    "even though we did not supply any group information to `Kmeans()`. We\n",
    "can plot the data, with each observation colored according to its cluster\n",
    "assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X[:,0], X[:,1], s = 50, c = kmeans.labels_, cmap = plt.cm.bwr) \n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:, 1], \n",
    "            marker = '*', \n",
    "            s = 150,\n",
    "            color = 'cyan', \n",
    "            label = 'Centers')\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the observations can be easily plotted because they are two-dimensional.\n",
    "If there were more than two variables then we could instead perform PCA\n",
    "and plot the first two principal components score vectors.\n",
    "\n",
    "In this example, we knew that there really were two clusters because\n",
    "we generated the data. However, for real data, in general we do not know\n",
    "the true number of clusters. We could instead have performed K-means\n",
    "clustering on this example with `K  =  3`. If we do this, K-means clustering will split up the two \"real\" clusters, since it has no information about them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_3_clusters = KMeans(n_clusters = 3, random_state = 123)\n",
    "kmeans_3_clusters.fit(X)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X[:,0], X[:,1], s=50, c=kmeans_3_clusters.labels_, cmap=plt.cm.prism) \n",
    "plt.scatter(kmeans_3_clusters.cluster_centers_[:, 0], kmeans_3_clusters.cluster_centers_[:, 1], marker='*', s=150,\n",
    "            color='blue', label='Centers')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the `Kmeans()` function in python with multiple initial cluster assignments,\n",
    "we use the `n_init` argument (default: 10). If a value of `n_init` greater than one\n",
    "is used, then K-means clustering will be performed using multiple random\n",
    "assignments, and the `Kmeans()` function will\n",
    "report only the best results. Here we compare using `n_init = 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km_out_single_run = KMeans(n_clusters = 3, n_init = 1, random_state = 123).fit(X)\n",
    "km_out_single_run.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to `n_init = 20`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km_out_single_run = KMeans(n_clusters = 3, n_init = 20, random_state = 123).fit(X)\n",
    "km_out_single_run.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `.inertia_` is the total within-cluster sum of squares,\n",
    "which we seek to minimize by performing K-means clustering.\n",
    "\n",
    "It is generally recommended to always run K-means clustering with a large\n",
    "value of `n_init`, such as 20 or 50 to avoid getting stuck in an undesirable local\n",
    "optimum.\n",
    "\n",
    "When performing K-means clustering, in addition to using multiple initial\n",
    "cluster assignments, it is also important to set a random seed using the\n",
    "`random_state` parameter. This way, the initial cluster assignments can\n",
    "be replicated, and the K-means output will be fully reproducible.\n",
    "\n",
    "# 10.5.2 Hierarchical Clustering\n",
    "The `linkage()` function from `scipy` implements several clustering functions in python. In the following example we use the data from the previous section to plot the hierarchical\n",
    "clustering dendrogram using complete, single, and average linkage clustering,\n",
    "with Euclidean distance as the dissimilarity measure. We begin by\n",
    "clustering observations using complete linkage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "hc_complete = linkage(X, \"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just as easily perform hierarchical clustering with average or single linkage instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hc_average = linkage(X, \"average\")\n",
    "hc_single = linkage(X, \"single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the dendrograms obtained using the usual `dendrogram()` function.\n",
    "The numbers at the bottom of the plot identify each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    hc_complete,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the cluster labels for each observation associated with a\n",
    "given cut of the dendrogram, we can use the `cut_tree()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cut_tree\n",
    "print(cut_tree(hc_complete, n_clusters = 2).T) # Printing transpose just for space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, complete and average linkage generally separates the observations\n",
    "into their correct groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.6: NCI60 Data Example\n",
    "\n",
    "Unsupervised techniques are often used in the analysis of genomic data. In this portion of the lab, we'll see how hierarchical and K-means clustering compare on the `NCI60` cancer cell line microarray data, which\n",
    "consists of 6,830 gene expression measurements on 64 cancer cell lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The NCI60 data\n",
    "nci_labs = pd.read_csv(\"NCI60_labs.csv\", index_col = 0)\n",
    "nci_data = pd.read_csv(\"NCI60_data.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell line is labeled with a cancer type. We'll ignore the\n",
    "cancer types in performing clustering, as these are unsupervised\n",
    "techniques. After performing clustering, we'll use this column to see the extent to which these cancer types agree with the results of these\n",
    "unsupervised techniques.\n",
    "\n",
    "The data has 64 rows and 6,830 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nci_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the cancer types for the cell lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nci_labs.x.value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.6.2 Clustering the Observations of the NCI60 Data\n",
    "We now proceed to hierarchically cluster the cell lines in the `NCI60` data,\n",
    "with the goal of finding out whether or not the observations cluster into\n",
    "distinct types of cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform hierarchical clustering of the observations using complete,\n",
    "single, and average linkage. We'll use standard Euclidean distance as the dissimilarity\n",
    "measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nci_data.index = nci_labs.x\n",
    "\n",
    "fig, ax = plt.subplots(3,1, figsize=(15,22))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "linkages = ['complete', 'single', 'average']\n",
    "for link, axis in zip(linkages, fig.axes):\n",
    "    hc = linkage(y = nci_data, method=link, metric='euclidean') \n",
    "    axis.set_title(\"Linkage=%s\" % link, size=15)\n",
    "    axis.set_xlabel('Sample Type', size=15)\n",
    "    axis.set_ylabel('Distance', size=15)\n",
    "    dendrogram(hc, ax=axis, labels=nci_data.index, leaf_rotation=90, leaf_font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the choice of linkage\n",
    "certainly does affect the results obtained. Typically, single linkage will tend\n",
    "to yield trailing clusters: very large clusters onto which individual observations\n",
    "attach one-by-one. On the other hand, complete and average linkage\n",
    "tend to yield more balanced, attractive clusters. For this reason, complete\n",
    "and average linkage are generally preferred to single linkage. Clearly cell\n",
    "lines within a single cancer type do tend to cluster together, although the\n",
    "clustering is not perfect. \n",
    "\n",
    "Let's use our complete linkage hierarchical clustering\n",
    "for the analysis. We can cut the dendrogram at the height that will yield a particular\n",
    "number of clusters, say 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nci_hc_complete = linkage(y = nci_data, method=\"complete\", metric='euclidean') \n",
    "\n",
    "nci_hc_complete_4_clusters = cut_tree(nci_hc_complete, n_clusters = 4) # Printing transpose just for space\n",
    "\n",
    "pd.crosstab(index = nci_data.index, \n",
    "            columns = nci_hc_complete_4_clusters.T[0], \n",
    "            rownames = ['Cancer Type'], \n",
    "            colnames = ['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some clear patterns. All the leukemia cell lines fall in cluster 2,\n",
    "while the breast cancer cell lines are spread out over three different clusters.\n",
    "We can plot the cut on the dendrogram that produces these four clusters by adding an `axhline()`, which draws a horizontal line on top of our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (15,8))\n",
    "dendrogram(nci_hc_complete, \n",
    "           labels = nci_data.index, \n",
    "           leaf_font_size = 14, \n",
    "           show_leaf_counts = True)  \n",
    "\n",
    "plt.axhline(y=110, c='k', ls='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We claimed earlier that K-means clustering and hierarchical\n",
    "clustering with the dendrogram cut to obtain the same number\n",
    "of clusters can yield **very** different results. How do these `NCI60` hierarchical\n",
    "clustering results compare to what we get if we perform K-means clustering\n",
    "with `K = 4`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmean_4 = KMeans(n_clusters = 4, random_state = 123, n_init = 150)    \n",
    "kmean_4.fit(nci_data)\n",
    "kmean_4.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a confusion matrix to compare the differences in how the two methods assigned observations to clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index = kmean_4.labels_, \n",
    "            columns = nci_hc_complete_4_clusters.T[0], \n",
    "            rownames = ['K-Means'], \n",
    "            colnames = ['Hierarchical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the four clusters obtained using hierarchical clustering and Kmeans\n",
    "clustering are somewhat different. Cluster 0 in K-means clustering is almost\n",
    "identical to cluster 2 in hierarchical clustering. However, the other clusters\n",
    "differ: for instance, cluster 2 in K-means clustering contains a portion of\n",
    "the observations assigned to cluster 0 by hierarchical clustering, as well as\n",
    "all of the observations assigned to cluster 1 by hierarchical clustering.\n",
    "\n",
    "To get credit for this lab, use a similar analysis to compare the results of your K-means clustering to the results of your hierarchical clustering with single and average linkage. What differences do you notice? Post your response to Moodle: https://moodle.smith.edu/mod/quiz/view.php?id=267171"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
